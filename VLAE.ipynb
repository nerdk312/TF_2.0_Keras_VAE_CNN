{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VLAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/TF_2.0_Keras_VAE_CNN/blob/master/VLAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DqoajHVwo3f",
        "colab_type": "code",
        "outputId": "e6a95f40-2e3a-4826-ef39-f65eaa726b89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-1.15.0\n",
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.17.4)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (41.6.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.2.7)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.7)\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed google-auth-1.7.1 tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kf8oeXvyU1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpxks2yr5lD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class conv2d_bn_lrelu(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_filters,kernel,stride):\n",
        "    super(conv2d_bn_lrelu,self).__init__()\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=kernel, strides=stride, activation=tf.identity)\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "    self.lrelu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "  \n",
        "  def call(self,inputs,training=None):\n",
        "    conv = self.conv1(inputs)\n",
        "    conv = self.bn(conv)\n",
        "    conv = self.lrelu(conv)\n",
        "    return conv\n",
        "\n",
        "class conv2d_t_bn_lrelu(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_filters, kernel,stride):\n",
        "    super(conv2d_t_bn_lrelu,self).__init__()\n",
        "    self.conv_t1 = tf.keras.layers.Conv2DTranspose(filters=num_filters,kernel_size=kernel,strides=stride,activation=tf.identity) \n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "    self.lrelu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "\n",
        "  def call(self,inputs,training=None):# Call the layers\n",
        "    convt = self.conv_t1(inputs)\n",
        "    convt = self.bn(convt)\n",
        "    convt = self.lrelu(convt)\n",
        "    return convt\n",
        "\n",
        "class conv2d_t_bn(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_filters, kernel,stride):\n",
        "    super(conv2d_t_bn,self).__init__()\n",
        "    self.conv_t1 = tf.keras.layers.Conv2DTranspose(filters=num_filters,kernel_size=kernel,strides=stride,activation=tf.identity) \n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "  def call(self,inputs,training=None):# Call the layers\n",
        "    convt = self.conv_t1(inputs)\n",
        "    convt = self.bn(convt)\n",
        "    return convt\n",
        "\n",
        "class fc_bn_lrelu(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_outputs):\n",
        "    super(fc_bn_lrelu,self).__init__()\n",
        "    self.fc = tf.keras.layers.Dense(num_outputs, activation=tf.identity)\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "    self.lrelu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "\n",
        "  def call(self,inputs,training=None):\n",
        "    fc1 =  self.fc(inputs)\n",
        "    fc1 = self.bn(fc1)\n",
        "    fc1 = self.lrelu(fc1)\n",
        "    return fc1  \n",
        "\n",
        "class fc_bn_relu(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_outputs):\n",
        "    super(fc_bn_relu,self).__init__()\n",
        "    self.fc = tf.keras.layers.Dense(num_outputs, activation=tf.identity)\n",
        "    self.bn = tf.keras.layers.BatchNormalization()\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "\n",
        "  def call(self,inputs,training=None):\n",
        "    fc1 =  self.fc(inputs)\n",
        "    fc1 = self.bn(fc1)\n",
        "    fc1 = self.relu(fc1)\n",
        "    return fc1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYLeO1qG0ME9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network:\n",
        "  def __init__(self,dataset,batch_size):\n",
        "    self.dataset = dataset\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    # These should be updated accordingly\n",
        "    self.iteration = 0\n",
        "    self.learning_rate = 0.0\n",
        "    self.read_only  = False\n",
        "    \n",
        "    self.do_generate_samples =  False\n",
        "    self.do_generate_conditional_samples = False\n",
        "    self.do_generate_manifold_samples = False\n",
        "  \n",
        "  def make_model_path(self):\n",
        "    if not os.path.isdir(\"models\"):\n",
        "      os.mkdir(\"models\")\n",
        "    if not os.path.isdir(\"models/\"+ self.name):\n",
        "      os.mkdir(\"models/\" + self.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIWgoDJjQcUU",
        "colab_type": "text"
      },
      "source": [
        "# vladder_small"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHgMN3xXV7_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SmallLayers(tf.keras.layers.Layer):\n",
        "  def __init__(self, network):\n",
        "    super(SmallLayers,self).__init__()\n",
        "    self.network = network\n",
        "\n",
        "class inference0(SmallLayers): # Nawid - Specifying this layer as a subclass of the other layer so I do not need to keep on writing, self.network\n",
        "  def __init__(self,network):\n",
        "    super(inference0,self).__init__(network) # Nawid - This is related to the parameters I need to pass into the parent \n",
        "    self.conv2d_bn_lrelu1 = conv2d_bn_lrelu(self.network.cs[1],4,2)\n",
        "    self.conv2d_bn_lrelu2 = conv2d_bn_lrelu(self.network.cs[2],4,2)\n",
        "    self.fc_layer = tf.keras.layers.Dense(self.network.cs[3], activation = tf.identity)\n",
        "\n",
        "  def call(self,input_x,training=None):\n",
        "    conv1 = self.conv2d_bn_lrelu1(input_x)\n",
        "    conv2 = self.conv2d_bn_lrelu2(conv1)\n",
        "    conv2 = tf.reshape(conv2, [-1, np.prod(conv2.get_shape().as_list()[1:])])\n",
        "    fc1 = self.fc_layer(conv2)\n",
        "    return fc1\n",
        "\n",
        "class ladder0(SmallLayers):\n",
        "  def __init__(self, network):\n",
        "    super(ladder0,self).__init__(network)\n",
        "    self.conv2d_bn_lrelu1 = conv2d_bn_lrelu(self.network.cs[1],4,2)\n",
        "    #print(self.network.cs[1])\n",
        "    self.conv2d_bn_lrelu2 = conv2d_bn_lrelu(self.network.cs[2],4,2)\n",
        "    self.fc_mean_layer = tf.keras.layers.Dense(self.network.ladder0_dim, activation = tf.identity)\n",
        "    self.fc_stddev_layer = tf.keras.layers.Dense(self.network.ladder0_dim, activation = 'sigmoid')\n",
        "\n",
        "  def call(self,input_x,input_size,ladder_dim,training=None): # Nawid - Added an extra parameter for the shape\n",
        "    conv1 = self.conv2d_bn_lrelu1(input_x)\n",
        "    conv2 = self.conv2d_bn_lrelu2(conv1)\n",
        "    conv2 = tf.reshape(conv2, [-1, np.prod(conv2.get_shape().as_list()[1:])])\n",
        "    fc1_mean = self.fc_mean_layer(conv2)\n",
        "    fc1_stddev = self.fc_stddev_layer(conv2)\n",
        "    # Nawid - Changes to include the loss function in the code\n",
        "    fc1_stddev += 0.001\n",
        "    fc1_sample = fc1_mean + tf.multiply(fc1_stddev, tf.keras.backend.random_normal(tf.stack([input_size, ladder_dim]))) \n",
        "\n",
        "    # Nawid - Computing the loss term related to this part\n",
        "    fc1_loss = -tf.math.log(fc1_stddev) + 0.5 * tf.math.square(fc1_stddev) + 0.5* tf.math.square(fc1_mean) - 0.5\n",
        "    fc1_loss = tf.reduce_mean(tf.reduce_sum(fc1_loss, axis = 1))\n",
        "    #self.add_loss(fc1_loss)\n",
        "\n",
        "    return fc1_sample, fc1_loss\n",
        "    #return fc1_mean, fc1_stddev\n",
        "\n",
        "class inference1(SmallLayers):\n",
        "  def __init__(self,network):\n",
        "    super(inference1,self).__init__(network)\n",
        "    self.fc_bn_lrelu1 = fc_bn_lrelu(self.network.cs[3])\n",
        "    self.fc_bn_lrelu2 = fc_bn_lrelu(self.network.cs[3])\n",
        "    self.fc_output = tf.keras.layers.Dense(self.network.cs[3], activation = tf.identity)\n",
        "\n",
        "  def call(self,latent1,training=None):\n",
        "    fc1 = self.fc_bn_lrelu1(latent1)\n",
        "    fc2 = self.fc_bn_lrelu2(fc1)\n",
        "    fc3 = self.fc_output(fc2)\n",
        "    return fc3\n",
        "\n",
        "class ladder1(SmallLayers):\n",
        "  def __init__(self,network):\n",
        "    super(ladder1, self).__init__(network)\n",
        "    self.fc_bn_lrelu1 = fc_bn_lrelu(self.network.cs[3])\n",
        "    self.fc_bn_lrelu2 = fc_bn_lrelu(self.network.cs[3])\n",
        "    self.fc_mean_layer = tf.keras.layers.Dense(self.network.ladder1_dim, activation= tf.identity)\n",
        "    self.fc_stddev_layer = tf.keras.layers.Dense(self.network.ladder1_dim, activation = 'sigmoid')\n",
        "    \n",
        "  def call(self,input_x,input_size,ladder_dim,training=None): # Nawid - input_x is latent1\n",
        "    fc1 = self.fc_bn_lrelu1(input_x)\n",
        "    fc2 = self.fc_bn_lrelu2(fc1)\n",
        "    fc3_mean = self.fc_mean_layer(fc2)\n",
        "    fc3_stddev = self.fc_stddev_layer(fc2)\n",
        "    fc3_stddev += 0.001\n",
        "    fc3_sample = fc3_mean + tf.multiply(fc3_stddev, tf.keras.backend.random_normal(tf.stack([input_size, ladder_dim]))) \n",
        "\n",
        "    # Nawid - Computing the loss term related to this part\n",
        "    fc3_loss = -tf.math.log(fc3_stddev) + 0.5 * tf.math.square(fc3_stddev) + 0.5* tf.math.square(fc3_mean) - 0.5\n",
        "    fc3_loss = tf.reduce_mean(tf.reduce_sum(fc3_loss, axis = 1))\n",
        "    #self.add_loss(fc3_loss)\n",
        "    return fc3_sample, fc3_loss\n",
        "\n",
        "class ladder2(SmallLayers):\n",
        "  def __init__(self,network):\n",
        "    super(ladder2, self).__init__(network)\n",
        "    self.fc_bn_lrelu1 = fc_bn_lrelu(self.network.cs[3])\n",
        "    self.fc_bn_lrelu2 = fc_bn_lrelu(self.network.cs[3])\n",
        "    self.fc_mean_layer = tf.keras.layers.Dense(self.network.ladder2_dim, activation= tf.identity)\n",
        "    self.fc_stddev_layer = tf.keras.layers.Dense(self.network.ladder2_dim, activation = 'sigmoid')\n",
        "\n",
        "  def call(self,input_x,input_size,ladder_dim,training=None): # Nawid -  input_x is latent2 \n",
        "    fc1 = self.fc_bn_lrelu1(input_x)\n",
        "    fc2 = self.fc_bn_lrelu2(fc1)\n",
        "    fc3_mean = self.fc_mean_layer(fc2)\n",
        "    fc3_stddev = self.fc_stddev_layer(fc2)\n",
        "    fc3_stddev += 0.001\n",
        "    fc3_sample = fc3_mean + tf.multiply(fc3_stddev, tf.keras.backend.random_normal(tf.stack([input_size, ladder_dim]))) \n",
        "\n",
        "    # Nawid - Computing the loss term related to this part\n",
        "    fc3_loss = -tf.math.log(fc3_stddev) + 0.5 * tf.math.square(fc3_stddev) + 0.5* tf.math.square(fc3_mean) - 0.5\n",
        "    fc3_loss = tf.reduce_mean(tf.reduce_sum(fc3_loss, axis = 1))\n",
        "    #self.add_loss(fc3_loss)\n",
        "    return fc3_sample, fc3_loss\n",
        "\n",
        "class generative0(SmallLayers):\n",
        "  def __init__(self, network):\n",
        "    super(generative0,self).__init__(network)\n",
        "  \n",
        "  def combine_noise(self,latent, ladder, method='gated_add', name='default'):\n",
        "    if method is 'concat':\n",
        "      return tf.concat(values = [latent,ladder], axis = len(latent.get_shape())-1)\n",
        "    else:\n",
        "      if method is 'add':\n",
        "        return latent + ladder\n",
        "      elif method is 'gated_add':\n",
        "        return latent + tf.multiply(self.gate, ladder)\n",
        "\n",
        "  def build(self,input_shape):    \n",
        "    self.gate = tf.Variable(tf.keras.backend.random_normal(shape=(input_shape[1:]))) # NAWID -input shape includes the batch, therefore I need to use values from past the zero element:\n",
        "    self.ladder0_present_fc_bn_lrelu = fc_bn_lrelu(self.network.cs[3])\n",
        "    self.fc_bn_relu1 = fc_bn_relu(int(self.network.fs[2]*self.network.fs[2]*self.network.cs[2]))\n",
        "    self.conv2d_t_bn_lrelu1 = conv2d_t_bn_lrelu(self.network.cs[1],2,2)\n",
        "    self.output_conv_t = tf.keras.layers.Conv2DTranspose(self.network.data_dims[-1],2,2, activation= 'sigmoid')\n",
        "    #self.conv2d_t_bn_lrelu1 = conv2d_t_bn_lrelu(self.network.cs[1],4,2, is_training)\n",
        "#    self.output_conv_t = tf.keras.layers.Conv2DTranspose(self.network.data_dims[-1],4,2, activation= 'sigmoid')\n",
        "\n",
        "  def call(self,input_x, ladder0,training=None): # Nawid  - Input_x corresponds to latent1    \n",
        "    ladder0 = self.ladder0_present_fc_bn_lrelu(ladder0)\n",
        "    latent1 = self.combine_noise(input_x, ladder0, name=\"generative0\")\n",
        "\n",
        "    fc1 = self.fc_bn_relu1(latent1)\n",
        "    #print(\"Before reshaped:\",fc1.shape)\n",
        "    fc1 = tf.reshape(fc1, tf.stack([tf.shape(fc1)[0], self.network.fs[2], self.network.fs[2], self.network.cs[2]]))\n",
        "    #print(\"Reshaped fc1\", fc1.shape)\n",
        "    conv1 = self.conv2d_t_bn_lrelu1(fc1)\n",
        "    #print(\"Conv1 shape\",conv1.shape)\n",
        "    output = self.output_conv_t(conv1)\n",
        "    #print(\"Output shape:\",output.shape)\n",
        "    output = (self.network.dataset.range[1] - self.network.dataset.range[0])* output + self.network.dataset.range[0]\n",
        "    #print(\"final output shape:\", output.shape)\n",
        "    '''\n",
        "    # Loss and training operators\n",
        "    reconstruction_loss =  tf.reduce_mean(tf.abs(output - original_input))\n",
        "    reg_coeff = 1.0\n",
        "    reconstruction_loss *= loss_ratio * np.prod(data_dims)\n",
        "    overall_loss = reg_coeff * regularisation + reconstruction_loss\n",
        "    self.add_loss(overall_loss)\n",
        "    '''\n",
        "    return output\n",
        "\n",
        " \n",
        "class generative1(SmallLayers):\n",
        "  def __init__(self, network):\n",
        "    super(generative1,self).__init__(network)\n",
        "\n",
        "  def combine_noise(self,latent, ladder, method='gated_add', name='default'):\n",
        "    if method is 'concat':\n",
        "      return tf.concat(values = [latent,ladder], axis = len(latent.get_shape())-1)\n",
        "    else:\n",
        "      if method is 'add':\n",
        "        return latent + ladder\n",
        "      elif method is 'gated_add':\n",
        "        return latent + tf.multiply(self.gate, ladder)\n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.gate = tf.Variable(tf.keras.backend.random_normal(shape=(input_shape[1:]))) # NAWID -input shape includes the batch, therefore I need to use values from past the zero element:    \n",
        "    self.ladder1_present_fc_bn_relu = fc_bn_relu(self.network.cs[3])\n",
        "    self.fc_bn_relu1 = fc_bn_relu(self.network.cs[3])\n",
        "    self.fc_bn_relu2 = fc_bn_relu(self.network.cs[3])\n",
        "    self.output_fc = tf.keras.layers.Dense(self.network.cs[3], activation= tf.identity)\n",
        "\n",
        "  def call(self,input_x,ladder1,training=None): # Nawid  - Input_x corresponds to latent1    \n",
        "    ladder1 = self.ladder1_present_fc_bn_relu(ladder1)\n",
        "    latent2 = self.combine_noise(input_x, ladder1, name=\"generative1\")\n",
        "    fc1 = self.fc_bn_relu1(latent2)\n",
        "    fc2 = self.fc_bn_relu2(fc1)\n",
        "    fc3 = self.output_fc(fc2)\n",
        "    return fc3\n",
        "\n",
        "class generative2(SmallLayers):\n",
        "  def __init__(self, network):\n",
        "    super(generative2,self).__init__(network)\n",
        "    self.fc_layer1 = fc_bn_relu(self.network.cs[3])\n",
        "    self.fc_layer2 = fc_bn_relu(self.network.cs[3])\n",
        "    self.fc_layer3 = tf.keras.layers.Dense(self.network.cs[3])#self.network.cs[3])\n",
        "  \n",
        "  def call(self,input_x,training=None): # Nawid -  In this case ladder 2 is in the input   \n",
        "    fc1 = self.fc_layer1(input_x)\n",
        "    fc2 = self.fc_layer2(fc1)\n",
        "    fc3 = self.fc_layer3(fc2)\n",
        "    return fc3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YtN17eE8GZi",
        "colab_type": "code",
        "outputId": "5771d63f-15fc-419b-b0d4-ea24bb1775b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "class VLadder(Network):\n",
        "  def __init__(self, dataset, name=None, reg='kl', batch_size = 100, restart= False):\n",
        "    super(VLadder,self).__init__(dataset, batch_size)\n",
        "    #Network.__init__(self, dataset, batch_size)\n",
        "    #print(batch_size)\n",
        "    if name is None or name == '':\n",
        "      self.name = \"vladder_%s\" % dataset.name # Nawid - GIves a name based on the dataset\n",
        "    else:\n",
        "      self.name = name\n",
        "\n",
        "    self.dataset = dataset\n",
        "    self.batch_size = batch_size\n",
        "    self.data_dims = self.dataset.data_dims \n",
        "\n",
        "    self.latent_noise = False\n",
        "    \n",
        "    self.fs = [self.data_dims[0], self.data_dims[0]//2, self.data_dims[0]//4 , self.data_dims[0]//8, self.data_dims[0]//16]\n",
        "    self.reg = reg\n",
        "    \n",
        "    if self.reg != 'kl':\n",
        "      print(' Unknown regularization, supported: kl')\n",
        "\n",
        "    if self.name == \"vladder_mnist\":\n",
        "      self.cs = [1, 64, 128, 1024] # Nawid-  Different number of cs and lower ladder dimension\n",
        "      self.ladder0_dim = 2\n",
        "      self.ladder1_dim = 2\n",
        "      self.ladder2_dim = 2\n",
        "      self.num_layers = 3\n",
        "      loss_ratio = 8.0\n",
        "      self.error_scale = 8.0\n",
        "      layers = SmallLayers(self) # Nawid- Uses small layers \n",
        "    else:\n",
        "      print('Unknown architecture name %s' % self.name)\n",
        "      exit(-1)\n",
        "  \n",
        "    self.self = self\n",
        "    '''\n",
        "    self_input_placeholder = self.dataset.train_data\n",
        "    input_size = self_input_placeholder.shape[0]\n",
        "    print(input_size)\n",
        "    '''\n",
        "    self_input_placeholder = tf.keras.layers.Input(shape= self.data_dims, dtype=tf.float32)  # Nawid - Input layer\n",
        "    input_size = tf.shape(self_input_placeholder)[0]\n",
        "    print('input_size1:',input_size)\n",
        "\n",
        "    # Define the inference network\n",
        "    self.regularisation = 0.0\n",
        "\n",
        "    if self.ladder0_dim>0:\n",
        "      ladder0_layer = ladder0(self) # Nawid - Self in this case is the network\n",
        "      \n",
        "      self.iladder0_sample, iladder0_loss = ladder0_layer(self_input_placeholder,input_size, self.ladder0_dim)\n",
        "      self.regularisation += iladder0_loss\n",
        "    \n",
        "    if self.num_layers >= 2:\n",
        "      inference0_layer = inference0(self)\n",
        "      self.ilatent1_hidden = inference0_layer(self_input_placeholder)      \n",
        "      if self.ladder1_dim >0:\n",
        "        ladder1_layer = ladder1(self)\n",
        "        self.iladder1_sample, iladder1_loss = ladder1_layer(self.ilatent1_hidden,input_size,self.ladder1_dim)\n",
        "        self.regularisation += iladder1_loss\n",
        "    \n",
        "    if self.num_layers >= 3:\n",
        "      inference1_layer = inference1(self)\n",
        "      self.ilatent2_hidden = inference1_layer(self.ilatent1_hidden)\n",
        "      if self.ladder2_dim >0:\n",
        "        ladder2_layer = ladder2(self)\n",
        "        self.iladder2_sample, iladder2_loss = ladder2_layer(self.ilatent2_hidden,input_size,self.ladder2_dim)\n",
        "        self.regularisation += iladder2_loss\n",
        "    \n",
        "    # Define the generative network\n",
        "#    self.ladders = {}\n",
        "    if self.num_layers >= 3 and self.ladder2_dim >0:\n",
        "      generative2_layer = generative2(self)\n",
        "      tlatent2_state = generative2_layer(self.iladder2_sample)\n",
        "          \n",
        "    if self.num_layers >= 2 and self.ladder1_dim >0:\n",
        "      generative1_layer = generative1(self)\n",
        "      tlatent1_state = generative1_layer(tlatent2_state, self.iladder1_sample)\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    if self.ladder0_dim > 0:\n",
        "      generative0_layer = generative0(self)\n",
        "      self.toutput = generative0_layer(tlatent1_state, self.iladder0_sample)\n",
        "    '''\n",
        "    #def call(self,input_x, ladder0, original_input,loss_ratio,data_dims,regularisation): # Nawid  - Input_x corresponds to latent1\n",
        "    # Loss and training operators\n",
        "\n",
        "    self.reconstruction_loss =  tf.reduce_mean(tf.abs(self.toutput - self_input_placeholder))\n",
        "    self.reg_coeff = 1.0\n",
        "\n",
        "    \n",
        "    if self.reg == 'kl':\n",
        "      self.reconstruction_loss *= loss_ratio * np.prod(self.data_dims)\n",
        "      self.loss = self.reg_coeff * self.regularization + self.reconstruction_loss\n",
        "    '''    \n",
        "\n",
        "    self.model = tf.keras.Model(self_input_placeholder,self.toutput)\n",
        "\n",
        "    self.model.summary()\n",
        "    tf.keras.utils.plot_model(self.model, 'my_new_model.png')\n",
        "    \n",
        "\n",
        "class Dataset:\n",
        "  def __init__(self):\n",
        "    self.name = \"abstract\"\n",
        "    self.data_dims = []\n",
        "    self.width = -1\n",
        "    self.height = -1\n",
        "    self.train_size = -1\n",
        "    self.test_size = -1\n",
        "    self.range = [0.0, 1.0]\n",
        "\n",
        "class MnistDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    super(MnistDataset,self).__init__()\n",
        "    (self.train_data, self.train_label), (self.test_data, self.test_label) = tf.keras.datasets.mnist.load_data()\n",
        "    self.train_data = self.train_data.astype('float32') #/ 255\n",
        "    self.train_data = self.train_data.reshape([-1, 28,28,1])\n",
        "    #self.train_data = self.train_data[0:5]\n",
        "    #print(self.train_data.shape)\n",
        "\n",
        "    self.test_data = self.test_data.astype('float32')#/255\n",
        "    self.test_data = self.test_data.reshape([-1, 28,28,1])\n",
        "\n",
        "    self.name = \"mnist\"\n",
        "    self.data_dims = [28,28,1]\n",
        "\n",
        "#    self.train_size = 50000\n",
        "#    self.test_size = 10000\n",
        "    self.range = [0.0,1.0]\n",
        "\n",
        "dataset = MnistDataset()\n",
        "train_data = dataset.train_data\n",
        "ladder_network = VLadder(dataset,batch_size=100) # Nawid -Need to define it manually so that the value does not change.\n",
        "VLAE =ladder_network.model\n",
        "regularisation = ladder_network.regularisation\n",
        "print(regularisation)\n",
        "#output = VLAE.predict(train_data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "input_size1: Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inference0 (inference0)         (None, 1024)         3410880     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Shape (TensorFlowOp [(4,)]               0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "inference1 (inference1)         (None, 1024)         3156992     inference0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [()]                 0           tf_op_layer_Shape[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "ladder2 (ladder2)               ((None, 2), ())      2111492     inference1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "generative2 (generative2)       (None, 1024)         2110464     ladder2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "ladder1 (ladder1)               ((None, 2), ())      2111492     inference0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "generative1 (generative1)       (None, 1024)         3165184     generative2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "ladder0 (ladder0)               ((None, 2), ())      145860      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "generative0 (generative0)       (None, 28, 28, 1)    6495425     generative1[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 22,707,789\n",
            "Trainable params: 22,669,773\n",
            "Non-trainable params: 38,016\n",
            "__________________________________________________________________________________________________\n",
            "Tensor(\"add_2:0\", shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54M2BavOFwOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_ratio = 8.0\n",
        "data_dims = [28,28,1]\n",
        "batch_size = 100 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4VEnkucFXjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def VLAE_loss(x,reconstructed_x):\n",
        "  reconstruction_loss =  tf.reduce_mean(tf.abs(reconstructed_x - x))\n",
        "  reg_coeff = 1.0\n",
        "  reconstruction_loss *= loss_ratio * np.prod(data_dims)\n",
        "  #overall_loss = reconstruction_loss\n",
        "  overall_loss = reg_coeff * regularisation + reconstruction_loss\n",
        "  print(regularisation) # Nawid- Print to show that this is changing each time\n",
        "  return overall_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKE91NegAgGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_weights_only=True, period = 5) # Nawid- Used to save every 5 epochs - Instance of the model checkpoint callback\n",
        "\n",
        "VLAE.compile(optimizer='adam',loss = VLAE_loss, experimental_run_tf_function=False)\n",
        "VLAE.fit(train_data, train_data,\n",
        "        shuffle=True,\n",
        "        epochs=10,\n",
        "        batch_size=100,\n",
        "        callbacks=[cp_callback])  # Pass callback to training))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9MSR7kro9_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir) # Nawid - Obtains the latest value of weights\n",
        "# Create a new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Load the previously saved weights\n",
        "model.load_weights(latest)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPK3PNqcHM--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_imgs = VLAE.predict(train_data)\n",
        "n = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(10):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_train[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dryh3WX_IQ6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}